defmodule Zoonk.AI.Evals.EvalFiles do
  @moduledoc """
  Utility functions to store and retrieve evaluation
  results for AI models and prompts.

  This module helps persist outputs and scores from
  LLM evaluations, avoiding duplicate processing and
  enabling comparison between models and prompts.

  It organizes the results in a structured directory
  format under `priv/evals`.
  """
  require Logger

  @type eval_type :: :model | :prompt

  @evals_dir "evals"
  @models_dir "models"
  @prompts_dir "prompts"
  @scores_dir "scores"

  @doc """
  Stores results generated by an AI model.

  We use these results to evaluate the model's performance
  and to compare it against other models.

  ## Examples

      iex> store_results(:model, "openai/gpt-4.1-mini", :suggest_courses, "outputs", "test_1.json", %{})
      :ok

      iex> store_results(:prompt, "openai/gpt-4.1-mini", :suggest_courses, "scores", "test_1.json", %{})
      :ok

  """
  @spec store_results(eval_type(), String.t(), atom(), String.t(), String.t(), map()) :: :ok
  def store_results(eval_type, model, prompt, results_dir, filename, data) do
    Logger.info("Storing #{eval_type} results for #{model} in #{filename}")

    eval_type
    |> results_dir!(model, prompt, results_dir)
    |> write_file!(filename, data)
  end

  @doc """
  Checks if a model file exists.

  This is useful to avoid sending duplicated requests
  to LLMs when we've already stored the results.

  ## Examples

      iex> file_exists?(:model, "openai/gpt-4.1-mini", :suggest_courses, "outputs", "test_1.json")
      true

      iex> file_exists?(:prompt, "openai/gpt-4.1-mini", :suggest_courses, "scores", "test_1.json")
      false
  """
  @spec file_exists?(eval_type(), String.t(), atom(), String.t(), String.t()) :: boolean()
  def file_exists?(eval_type, model, prompt, results_dir, filename) do
    eval_type
    |> results_dir!(model, prompt, results_dir)
    |> Path.join(filename)
    |> File.exists?()
  end

  @doc """
  Loads all score files for a prompt.

  This function reads all JSON files from the scores directory
  for a given prompt and returns their parsed content.

  ## Examples

      iex> load_prompt_scores(:suggest_courses)
      [%{"usage" => %{...}, "steps" => [...]}, ...]
  """
  @spec load_prompt_scores(atom() | String.t()) :: [map()]
  def load_prompt_scores(prompt) do
    prompt_name = prompt_name(prompt)
    scores_dir = Path.join(["priv", @evals_dir, @prompts_dir, prompt_name, @scores_dir])
    load_scores_from_dir(scores_dir, "prompt: #{prompt_name}")
  end

  @doc """
  Loads all score files for a model and prompt.

  This function reads all JSON files from the scores directory
  for a given model and prompt and returns their parsed content.

  ## Examples

      iex> load_model_scores(:suggest_courses, "deepseek-chat-v3-0324")
      [%{"usage" => %{...}, "steps" => [...]}, ...]
  """
  @spec load_model_scores(atom() | String.t(), String.t()) :: [map()]
  def load_model_scores(prompt, model) do
    prompt_name = prompt_name(prompt)
    model_name = model_name(model)
    scores_dir = Path.join(["priv", @evals_dir, @models_dir, model_name, prompt_name, @scores_dir])
    load_scores_from_dir(scores_dir, "model: #{model_name}, prompt: #{prompt_name}")
  end

  defp load_scores_from_dir(scores_dir, context) do
    case File.ls(scores_dir) do
      {:ok, files} ->
        files
        |> filter_json_files()
        |> parse_score_files(scores_dir)

      {:error, _reason} ->
        Logger.warning("No scores directory found for #{context}")
        []
    end
  end

  defp filter_json_files(files) do
    Enum.filter(files, &String.ends_with?(&1, ".json"))
  end

  defp parse_score_files(json_files, scores_dir) do
    json_files
    |> Enum.map(&load_score_file(scores_dir, &1))
    |> Enum.reject(&is_nil/1)
  end

  @doc """
  Updates the markdown file with the calculated scores for a prompt.

  This function creates or updates a markdown file in `priv/evals/{prompt_name}.md`
  with the average and median scores.

  ## Examples

      iex> update_scores_markdown(:suggest_courses, %{average: 7.76, median: 9.0})
      :ok
  """
  @spec update_scores_markdown(%{average: float(), median: float()}, atom() | String.t()) :: :ok
  def update_scores_markdown(%{average: average, median: median}, prompt_name) do
    prompt_str = prompt_name(prompt_name)
    file_path = Path.join(["priv", @evals_dir, "#{prompt_str}.md"])
    title = score_file_title(prompt_str)

    content = """
    ## #{title}

    - **Average score**: #{average}
    - **Median score**: #{median}
    """

    update_markdown_section(file_path, title, content)
  end

  @doc """
  Updates the markdown file with the model leaderboard.

  This function creates or updates the leaderboard section in the markdown file
  for a given prompt with sorted model scores.

  ## Examples

      iex> update_leaderboard_markdown(%{"model1" => %{average: 8.0, median: 9.0}}, :suggest_courses)
      :ok
  """
  @spec update_leaderboard_markdown(map(), atom() | String.t()) :: :ok
  def update_leaderboard_markdown(leaderboard, prompt_name) do
    prompt_str = prompt_name(prompt_name)
    file_path = Path.join(["priv", @evals_dir, "#{prompt_str}.md"])
    title = "Model Leaderboard"

    leaderboard_content = generate_leaderboard_content(leaderboard)
    update_markdown_section(file_path, title, leaderboard_content)
  end

  defp update_markdown_section(file_path, title, content) do
    maybe_create_dir(file_path)

    if File.exists?(file_path) do
      update_existing_markdown(file_path, content, title)
    else
      File.write!(file_path, content)
    end
  end

  @doc """
  Updates the leaderboard JSON file with model scores.

  This function creates or updates a JSON file in `priv/evals/{prompt_name}_leaderboard.json`
  with the model scores.

  ## Examples

      iex> update_leaderboard_json(%{average: 7.76, median: 9.0}, :suggest_courses, "deepseek-chat-v3-0324")
      %{"deepseek-chat-v3-0324" => %{average: 7.76, median: 9.0}}
  """
  @spec update_leaderboard_json(map(), atom() | String.t(), String.t()) :: map()
  def update_leaderboard_json(model_scores, prompt, model) do
    prompt_str = prompt_name(prompt)
    path = Path.join(["priv", @evals_dir, "#{prompt_str}_leaderboard.json"])

    current = read_file_or_default!(path, %{})
    updated = Map.put(current, model_name(model), model_scores)
    write_json!(path, updated)
    updated
  end

  defp score_file_title(prompt_name) do
    prompt_name
    |> String.split("_")
    |> Enum.map_join(" ", &String.capitalize/1)
  end

  defp results_dir!(:model, model, prompt, results_dir) do
    model_name = model_name(model)
    prompt = prompt_name(prompt)

    Path.join(["priv", @evals_dir, @models_dir, model_name, prompt, results_dir])
  end

  defp results_dir!(:prompt, _model, prompt, results_dir) do
    prompt = prompt_name(prompt)

    Path.join(["priv", @evals_dir, @prompts_dir, prompt, results_dir])
  end

  defp maybe_create_dir(file_path) do
    file_path
    |> Path.dirname()
    |> File.mkdir_p!()
  end

  defp write_file!(dir, filename, data) do
    File.mkdir_p!(dir)

    file_path = Path.join(dir, filename)
    file_content = Jason.encode!(data, pretty: true)

    File.write!(file_path, file_content)

    Logger.info("Stored results in #{file_path}")
  end

  defp read_file_or_default!(path, default) do
    if File.exists?(path) do
      path
      |> File.read!()
      |> Jason.decode!()
    else
      default
    end
  end

  defp write_json!(path, data) do
    maybe_create_dir(path)
    File.write!(path, Jason.encode!(data, pretty: true))
    :ok
  end

  defp load_score_file(scores_dir, filename) do
    scores_dir
    |> Path.join(filename)
    |> File.read!()
    |> Jason.decode!()
  end

  defp update_existing_markdown(file_path, new_content, title) do
    existing = File.read!(file_path)
    updated = upsert_section(existing, title, new_content)
    File.write!(file_path, updated)
  end

  defp upsert_section(content, title, new_section) do
    if section_exists?(content, title) do
      replace_section(content, title, new_section)
    else
      append_section(content, new_section)
    end
  end

  defp section_exists?(content, title) do
    String.contains?(content, "## #{title}")
  end

  defp replace_section(content, title, new_section) do
    title
    |> section_regex()
    |> Regex.replace(content, new_section)
  end

  defp append_section(content, new_section) do
    content <> "\n\n" <> new_section
  end

  defp section_regex(title) do
    ~r/## #{Regex.escape(title)}.*?(?=\n## |\z)/s
  end

  defp prompt_name(prompt) when is_atom(prompt), do: Atom.to_string(prompt)
  defp prompt_name(prompt) when is_binary(prompt), do: prompt

  defp model_name(model) do
    model
    |> String.split("/", parts: 2)
    |> List.last()
    |> String.replace("/", "_")
    |> String.replace_suffix(":free", "")
  end

  defp generate_leaderboard_content(leaderboard) do
    """
    ## Model Leaderboard

    | Model                 | Average | Median |
    | --------------------- | ------- | ------ |
    #{leaderboard_rows(leaderboard)}

    """
  end

  defp leaderboard_rows(leaderboard) do
    leaderboard
    |> sort_leaderboard_entries()
    |> format_leaderboard_rows()
  end

  defp sort_leaderboard_entries(leaderboard) do
    leaderboard
    |> Enum.map(&normalize_scores/1)
    |> Enum.sort_by(fn entry -> {entry.average, entry.median} end, :desc)
  end

  defp format_leaderboard_rows(entries) do
    Enum.map_join(entries, "\n", &format_row/1)
  end

  defp normalize_scores({model, scores}) do
    %{
      model: model,
      average: get_score(scores, :average),
      median: get_score(scores, :median)
    }
  end

  defp get_score(scores, key) when is_map(scores) do
    Map.get(scores, key) || Map.get(scores, to_string(key), 0)
  end

  defp format_row(%{model: model, average: avg, median: med}) do
    "| #{String.pad_trailing(model, 21)} | #{String.pad_trailing(to_string(avg), 7)} | #{String.pad_trailing(to_string(med), 6)} |"
  end
end
